name: CI/CD Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]

env:
  NODE_VERSION: '18'
  PYTHON_VERSION: '3.11'

jobs:
  # Backend Tests and Quality Checks
  backend-test:
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:16
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_USER: postgres
          POSTGRES_DB: valuerpro_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install backend dependencies
      working-directory: ./backend
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev]
    
    - name: Run backend linting
      working-directory: ./backend
      run: |
        black --check app/ tests/
        isort --check-only app/ tests/
        flake8 app/ tests/
    
    - name: Run backend type checking
      working-directory: ./backend
      run: |
        mypy app/
    
    - name: Run backend security scan
      working-directory: ./backend
      run: |
        bandit -r app/ -f json -o bandit-report.json || true
        safety check --json --output safety-report.json || true
    
    - name: Run AI-enhanced backend tests
      working-directory: ./backend
      env:
        DATABASE_URL: postgresql://postgres:postgres@localhost:5432/valuerpro_test
        SECRET_KEY: test-secret-key-for-ci-cd-only
        ENVIRONMENT: test
      run: |
        # Run unit tests with coverage
        pytest --cov=app --cov-report=xml --cov-report=html tests/ -v
        
        # Run property-based tests with Hypothesis
        pytest tests/ -k "hypothesis" -v || true
        
        # Run performance benchmarks
        pytest tests/ -k "benchmark" --benchmark-json=benchmark.json -v || true
        
        # Run AI-generated edge case tests
        pytest tests/ -k "ai_generated" -v || true
    
    - name: Upload performance benchmarks
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: backend-performance-benchmarks
        path: |
          backend/benchmark.json
          backend/.benchmarks/
    
    - name: Upload backend coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./backend/coverage.xml
        flags: backend
        name: backend-coverage

    - name: Upload backend security reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: backend-security-reports
        path: |
          backend/bandit-report.json
          backend/safety-report.json

  # Frontend Tests and Quality Checks with AI
  frontend-test:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
        cache-dependency-path: frontend/package-lock.json
    
    - name: Install frontend dependencies
      working-directory: ./frontend
      run: npm ci
    
    - name: Install Playwright browsers
      working-directory: ./frontend
      run: npm run playwright:install
    
    - name: Run frontend linting
      working-directory: ./frontend
      run: |
        npm run lint
        npm run type-check
    
    - name: Run unit tests with coverage
      working-directory: ./frontend
      run: npm run test:ci
      env:
        CI: true
    
    - name: Start backend for E2E tests
      working-directory: ./backend
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev]
        uvicorn app.main:app --host 0.0.0.0 --port 8000 &
        sleep 10
      env:
        DATABASE_URL: postgresql://postgres:postgres@localhost:5432/valuerpro_test
        SECRET_KEY: test-secret-key-for-ci-cd-only
        ENVIRONMENT: test
    
    - name: Start frontend for E2E tests
      working-directory: ./frontend
      run: |
        npm run build
        npm run start &
        sleep 15
        curl --retry 10 --retry-delay 3 --retry-connrefused http://localhost:3000
      env:
        NEXT_PUBLIC_API_URL: http://localhost:8000
    
    - name: Run AI-powered E2E tests
      working-directory: ./frontend
      run: npm run test:e2e
      env:
        CI: true
        APPLITOOLS_API_KEY: ${{ secrets.APPLITOOLS_API_KEY }}
    
    - name: Run AI-powered visual regression tests
      working-directory: ./frontend
      run: npm run test:visual
      env:
        CI: true
        APPLITOOLS_API_KEY: ${{ secrets.APPLITOOLS_API_KEY }}
      continue-on-error: true
    
    - name: Run comprehensive accessibility tests
      working-directory: ./frontend
      run: npm run test:accessibility
      env:
        CI: true
    
    - name: Run performance benchmarks
      working-directory: ./frontend
      run: |
        npm run lighthouse:audit || true
        npm run test:e2e -- --grep "@performance"
      continue-on-error: true
    
    - name: Upload test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: frontend-test-results
        path: |
          frontend/test-results/
          frontend/reports/
          frontend/playwright-report/
    
    - name: Upload frontend coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./frontend/coverage/lcov.info
        flags: frontend
        name: frontend-coverage

  # AI-Powered Test Quality Analysis
  ai-test-analysis:
    runs-on: ubuntu-latest
    needs: [backend-test, frontend-test]
    if: always()
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python for AI analysis
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Download test results
      uses: actions/download-artifact@v3
      with:
        name: frontend-test-results
        path: ./test-results/frontend/
    
    - name: Download backend benchmarks
      uses: actions/download-artifact@v3
      with:
        name: backend-performance-benchmarks
        path: ./test-results/backend/
      continue-on-error: true
    
    - name: Install AI analysis dependencies
      run: |
        pip install pandas numpy matplotlib seaborn
        pip install scikit-learn openai
    
    - name: Run AI test quality analysis
      run: |
        cat > ai_test_analyzer.py << 'EOF'
        import json
        import os
        import pandas as pd
        from pathlib import Path
        
        def analyze_test_results():
            results = {
                "overall_quality_score": 0,
                "insights": [],
                "recommendations": [],
                "trends": {}
            }
            
            # Analyze Playwright results
            playwright_results_path = Path("test-results/frontend/results.json")
            if playwright_results_path.exists():
                with open(playwright_results_path) as f:
                    playwright_data = json.load(f)
                
                total_tests = len(playwright_data.get('suites', []))
                passed_tests = sum(1 for suite in playwright_data.get('suites', []) 
                                 if suite.get('outcome') == 'passed')
                
                pass_rate = passed_tests / total_tests if total_tests > 0 else 0
                results["trends"]["e2e_pass_rate"] = pass_rate
                
                if pass_rate > 0.9:
                    results["insights"].append("Excellent E2E test pass rate")
                    results["overall_quality_score"] += 25
                elif pass_rate > 0.8:
                    results["insights"].append("Good E2E test pass rate")
                    results["overall_quality_score"] += 20
                else:
                    results["recommendations"].append("E2E test pass rate below 80% - investigate failures")
            
            # Analyze performance benchmarks
            benchmark_path = Path("test-results/backend/benchmark.json")
            if benchmark_path.exists():
                with open(benchmark_path) as f:
                    benchmark_data = json.load(f)
                
                # Analyze performance trends
                benchmarks = benchmark_data.get('benchmarks', [])
                if benchmarks:
                    avg_time = sum(b.get('stats', {}).get('mean', 0) for b in benchmarks) / len(benchmarks)
                    results["trends"]["avg_performance_time"] = avg_time
                    
                    if avg_time < 0.1:  # Under 100ms average
                        results["insights"].append("Excellent API performance")
                        results["overall_quality_score"] += 25
                    elif avg_time < 0.5:  # Under 500ms average
                        results["insights"].append("Good API performance")
                        results["overall_quality_score"] += 15
                    else:
                        results["recommendations"].append("API performance above 500ms - optimization needed")
            
            # AI-powered pattern detection
            results["ai_insights"] = detect_patterns()
            
            # Calculate final quality score
            results["overall_quality_score"] = min(100, results["overall_quality_score"])
            
            return results
        
        def detect_patterns():
            patterns = []
            
            # Pattern 1: Test naming consistency
            test_files = list(Path(".").glob("**/tests/**/*.py")) + list(Path(".").glob("**/tests/**/*.spec.ts"))
            
            naming_patterns = {}
            for file in test_files:
                if file.name.startswith('test_'):
                    naming_patterns['pytest'] = naming_patterns.get('pytest', 0) + 1
                elif file.name.endswith('.spec.ts'):
                    naming_patterns['playwright'] = naming_patterns.get('playwright', 0) + 1
            
            if len(naming_patterns) > 0:
                patterns.append(f"Test naming patterns: {naming_patterns}")
            
            # Pattern 2: Test coverage analysis
            patterns.append("AI recommends focusing on edge cases and error handling")
            patterns.append("Consider adding more property-based tests for data validation")
            
            return patterns
        
        if __name__ == "__main__":
            analysis = analyze_test_results()
            
            print("ðŸ¤– AI Test Quality Analysis Report")
            print("=" * 50)
            print(f"Overall Quality Score: {analysis['overall_quality_score']}/100")
            print()
            
            if analysis['insights']:
                print("âœ… Key Insights:")
                for insight in analysis['insights']:
                    print(f"  â€¢ {insight}")
                print()
            
            if analysis['recommendations']:
                print("ðŸ“‹ Recommendations:")
                for rec in analysis['recommendations']:
                    print(f"  â€¢ {rec}")
                print()
            
            if analysis['ai_insights']:
                print("ðŸ§  AI Pattern Analysis:")
                for pattern in analysis['ai_insights']:
                    print(f"  â€¢ {pattern}")
            
            # Save results for further analysis
            with open('ai_test_analysis.json', 'w') as f:
                json.dump(analysis, f, indent=2)
        EOF
        
        python ai_test_analyzer.py
    
    - name: Upload AI analysis results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: ai-test-analysis
        path: ai_test_analysis.json
    
    - name: Comment AI analysis on PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          
          let analysisComment = '## ðŸ¤– AI Test Quality Analysis\n\n';
          
          try {
            const analysis = JSON.parse(fs.readFileSync('ai_test_analysis.json', 'utf8'));
            
            analysisComment += `**Quality Score:** ${analysis.overall_quality_score}/100\n\n`;
            
            if (analysis.insights && analysis.insights.length > 0) {
              analysisComment += '### âœ… Key Insights\n';
              analysis.insights.forEach(insight => {
                analysisComment += `- ${insight}\n`;
              });
              analysisComment += '\n';
            }
            
            if (analysis.recommendations && analysis.recommendations.length > 0) {
              analysisComment += '### ðŸ“‹ Recommendations\n';
              analysis.recommendations.forEach(rec => {
                analysisComment += `- ${rec}\n`;
              });
            }
            
          } catch (error) {
            analysisComment += 'Analysis data not available - check test execution logs.';
          }
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: analysisComment
          });

  # Security Scanning
  security-scan:
    runs-on: ubuntu-latest
    if: github.event_name == 'push'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        scan-ref: '.'
        format: 'sarif'
        output: 'trivy-results.sarif'
    
    - name: Upload Trivy scan results to GitHub Security tab
      uses: github/codeql-action/upload-sarif@v2
      if: always()
      with:
        sarif_file: 'trivy-results.sarif'
    
    - name: Run CodeQL Analysis
      uses: github/codeql-action/init@v2
      with:
        languages: javascript, python
    
    - name: Autobuild
      uses: github/codeql-action/autobuild@v2
    
    - name: Perform CodeQL Analysis
      uses: github/codeql-action/analyze@v2

  # Build and Package
  build:
    runs-on: ubuntu-latest
    needs: [backend-test, frontend-test]
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Login to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ghcr.io
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Build and push backend Docker image
      uses: docker/build-push-action@v5
      with:
        context: ./backend
        push: true
        tags: |
          ghcr.io/${{ github.repository }}/backend:latest
          ghcr.io/${{ github.repository }}/backend:${{ github.sha }}
        cache-from: type=gha
        cache-to: type=gha,mode=max
    
    - name: Build and push frontend Docker image
      uses: docker/build-push-action@v5
      with:
        context: ./frontend
        push: true
        tags: |
          ghcr.io/${{ github.repository }}/frontend:latest
          ghcr.io/${{ github.repository }}/frontend:${{ github.sha }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

  # Deploy to Staging
  deploy-staging:
    runs-on: ubuntu-latest
    needs: [build]
    if: github.ref == 'refs/heads/develop'
    environment: staging
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Deploy to staging
      run: |
        echo "Deploying to staging environment..."
        # Add actual deployment commands here
        # Example: kubectl apply -f k8s/staging/
    
    - name: Run staging health checks
      run: |
        echo "Running staging health checks..."
        # Add health check commands here
    
    - name: Run staging integration tests
      run: |
        echo "Running staging integration tests..."
        # Add integration test commands here

  # Deploy to Production
  deploy-production:
    runs-on: ubuntu-latest
    needs: [build]
    if: github.ref == 'refs/heads/main'
    environment: production
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Deploy to production
      run: |
        echo "Deploying to production environment..."
        # Add actual deployment commands here
        # Example: kubectl apply -f k8s/production/
    
    - name: Run production health checks
      run: |
        echo "Running production health checks..."
        # Add health check commands here
    
    - name: Run production smoke tests
      run: |
        echo "Running production smoke tests..."
        # Add smoke test commands here
    
    - name: Notify deployment success
      uses: 8398a7/action-slack@v3
      if: success()
      with:
        status: success
        channel: '#deployments'
        text: 'Production deployment successful! :rocket:'
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
    
    - name: Notify deployment failure
      uses: 8398a7/action-slack@v3
      if: failure()
      with:
        status: failure
        channel: '#deployments'
        text: 'Production deployment failed! :warning:'
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}

  # Dependency Updates
  dependency-check:
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Check for outdated dependencies
      run: |
        echo "Checking for outdated dependencies..."
        # Add dependency checking commands
    
    - name: Create dependency update PR
      uses: peter-evans/create-pull-request@v5
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        commit-message: 'chore: update dependencies'
        title: 'Automated Dependency Updates'
        body: 'This PR updates outdated dependencies to their latest versions.'
        branch: 'dependency-updates'